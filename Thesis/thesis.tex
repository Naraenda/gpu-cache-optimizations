\documentclass{article}

\usepackage[sort&compress,numbers]{natbib}
\usepackage{multicol}
\usepackage{etoolbox}
\usepackage{subfig}
\usepackage{tikz}
    \usetikzlibrary{colorbrewer}
    \usetikzlibrary{patterns}
    \usetikzlibrary{spy}
    \usetikzlibrary{ decorations.markings}
\usepackage{float}
\usepackage[framemethod=default]{mdframed}
\usepackage[a4paper, margin=1in]{geometry}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{pgfplots}
    \usepgfplotslibrary{colorbrewer}
    \pgfplotsset{compat=1.18}
\usepackage{minted}
    \setminted[haskell]{
        frame=lines,
        framesep=2mm,
        linenos
    }
\usepackage{hhline}
\usepackage{amsmath}
\usepackage{amssymb}

\setlist{noitemsep, topsep=0pt}

\newcommand{\TODO}[1]{\noindent{\color{red}\textbf{[TODO] #1}}}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newrobustcmd*{\fsquare}[1]{\tikz{\filldraw[draw=#1,fill=#1] (0,0) rectangle (0.22cm,0.22cm);}}

\title{Improving Performance in Structured GPGPU Workloads via Specialized Thread Schedules}
\author{Naraenda Prasetya}
\date{2022}

\begin{document}

\maketitle

\section*{Abstract}
\TODO{Write thisss}

\tableofcontents

\TODO{Make sure equations are consitent:}
\begin{itemize}
    \item $L$ Cache line size (elements)
    \item $M$ Cache size (elements)
    \item $M_L$ Cache size (number of cache lines)
    \item $F$ Memory fetches
    \item $I_w$, $I_h$ input size (number of tasks to produce the output)
    \item $S_w$, $S_w$ stencil size
\end{itemize}

\section{Introduction}

Graphics Processing Units (GPUs) are specialized hardware that can manipulate large amounts of data in a highly parallel manner.
The hardware's main purpose is processing graphical data, however modern use of GPUs is in the form of general-pupose computing on the GPU (GPGPU) where we exploit the GPUs many cores to run computations normally run on CPUs in parallel.

This high performance GPU computing has become very accessible via high level frameworks, which provide a limited set of operators, such as stencil, permute, fold, scan, which can manipulate data on a GPU \cite{chakravarty2011accelerating}.
However, in most cases the compiled assembly has inefficient memory accesses with lower cache hit rates and uncoalesced accesses than manually tweaked code.
Threads can be scheduled in such a way to minimize these inefficiencies.
In such cases, a smarter scheduler can achieve better cache and memory utilization \cite{nugteren2014study}.
We propose a simple schedule to improve performance by leveraging the structure of memory accesses in the high level GPGPU framework Accelerate \cite{chakravarty2011accelerating}.

\subsection{Motivation}



\subsection{Contributions}
This thesis shows that the cache with tiled implementations for stencil and matrix multiplication can be improved by rescheduling via index mapping.
While the main focus lies in improving the performance on GPU, the techniques presented can also be applied on a CPU.

\vspace{1em}
The main contributions of this thesis are:
\begin{itemize}
    \item An analysis for the theoretical cache utilization and efficiency of both linear and multithreaded (GPU) execution for naive and tiled implementations of stencil and matrix multiplication operations.
    \item A simple implementeable schedule for stencil and matrix multiplication operations that better utilize the cache, which results in equal or better performance than the naive and tiled implementation.
    \item An analysis for the theoretical cache utilization and efficiency of a column-based scheduler.
\end{itemize}

\subsection{Outline}
\TODO{Chapter x introduces yada, yada}

\section{Background}

\subsection{GPU Architecture}
Since the GPU backend of Accelerate only works with CUDA capable devices (see section \ref{sec:accelerate}), we will mostly focus on the architecture of newer NVidia GPUs.
Massive parallel workloads are executed on numerous cores clustered in streaming multiprocessors (SMs).
The memory is structured in a multi-level hierarchy containing an L1 cache for each SM, a shared L2 cache for all SMs and multiple banks of DRAM \cite{nvidia2017volta,nvidia2020ampere}.

When working with CUDA the programmer defines a kernel.
This is normally done with CUDA C++, an extension on C++ programming language, but in our case Accelerate will handle the generation of kernels (section \ref{sec:accelerate}) \cite{nvidia2021cudadocs}.

\subsubsection{Hardware}
On modern Nvidia GPU architectures the compute units are grouped in graphics processing clusters (GPC).
Since GPUs are primarily used in graphics applications, the GPC contains some specialized components: a raster engine, Texture Processing Clusters (TPCs), \TODO{\dots}.
The main point of interest are streaming multiprocessors (SMs) in the TPCs.
Each SMs has its own instruction scheduler and various execution pipelines.
\citeauthor{jia2019dissecting} suggests that a threadblock should contain at least 128 threads due to SMs on Turing, Volta, and Ampere being split into four processing blocks \cite{jia2019dissecting,nvidia2017volta,nvidia2018turing,nvidia2020ampere}.

\TODO{More specific + figure}

\subsubsection{Software}
Executions on a GPU are directed on both the host and device (GPU).
\textit{Kernels} define the functions that should be executed on the GPU.
The host side controls how these kernels should be executed, namely how the threads should be launched and executed.
Threads are grouped and defined on a 2 level hierarchy: threads are grouped together in \textit{cooperative thread arrays} (CTAs), also known as thread blocks, and multiple CTAs can be queued for the execution of a single kernel.
Both can be controlled upon executing a kernel: the amount of threads per CTA (threadblock size) and the total amount of CTAs (gridsize).
CTAs get assigned to SMs in a round-robin fashion.

When an SM executes a CTA, it splits the work into warps, a grouping of 32-threads.
On architecture before Volta, a single warp is executed in a single instruction, multiple threads fashion, where a single program counter is shared amongst the 32 threads.
With the volta architecture, independent thread scheduling allows full concurency between threads and the scheduler can group multiple threads into SIMT units.

\TODO{SM can context switch between multipel warps to hide latency}

\TODO{Figure visualizing the whole thing because text is confusing... probably}

\subsubsection{GPU Caches}
\label{sec:cache_gpu}
To the programmer, the memory hierarchy is very simplified: there is the compute unit and the memory.
Caches are hidden from this model as on most architectures they are managed by hardware.

Memory can become a significant bottleneck due to the large amount of threads running concurrently.
Caches can alleviate this but are limited in size, and given a large enough problem can cause cache trashing -- the premature eviction of data before any significant reuse \cite{dai2016model}.
To improve efficiency of caches, caches asume spatial locality via cache lines.
A cache line is the smallest unit of data that a cache can hold.
For example, an L1 cache on a Turing GPU uses cache lines that hold 128 bytes of data.
Therefore, fetching data from memory also brings extra nearby data with it.

Data shared between threads through the cache can happen in a read-after-write (RAW) or read-after-read (RAR) manner.
RAW has data dependency among tasks, for example in scan operations.
RAR has no data dependency and can be executed in any order \cite{tripathy2021paver}.

The L1 cache in older Nvidia GPU architectures (Maxwell, Pascal) uses the least recently used (LRU) eviction policy.
When caches become full, we need to remove data (a cache line) from the cache to allow newer data to be cached.
An LRU eviction policy evicts data that is the least recently used.
\citet{jia2019dissecting} have shown that in Turing and Volta GPUs, the P-chase benchmark that is used to detect the LRU eviction policy presented by \citet{mei2016dissecting} fails to complete over the full L1 cache.
\citeauthor{jia2019dissecting} conclude that newer architectures (Turing, Volta) uses a non-LRU eviction policy \cite{jia2019dissecting, jia2018dissecting,mei2016dissecting}.
When the L1 cache in Turing and Volta GPU saturates, 4 consecutive cache lines are chosen randomly to be evicted.
This is in line with a new eviction policy mechanism introduced with Volta, where cache lines can be assigned a priority \cite{jia2019dissecting,nvidia2021cudadocs}.

Modern Nvidia GPUs are able to handle various types cache operations and eviction hints.
By default, loads are cached at all levels (L2, L1) with an LRU policy.
This brings a problem with it: if data is writen to a cached value, we need to evict this cache line from all other L1 caches first, since that value is no longer up to date after our update.
As an example, it is also possible to only cache on L2, bypassing L1.
Another option is to hint cache streaming, where the loaded cache line will have an evict-first policy to prevent polution of the cache.
Similar operations exist for writing data to memory.
In both cases it is up to the compiler and programmer to exploit this for extra performance \cite{nvidia2021cudadocs}.

\subsubsection{Performance of Access Patterns}
\citeauthor{lam1991cache} and \citeauthor{meyer2003algorithms} describe two types of reference reuse\cite{lam1991cache, meyer2003algorithms}:
\begin{itemize}
    \item \textbf{Spatial reuse} occurs when accessing data from the same cache line, increasing spatial locality.
    \item \textbf{Temporal reuse} occurs when the same data is accessed at a later time, increasing temporal locality.
\end{itemize}
The reuse factor can be kept track of by counting the two types of reuse.
Loading data horizontally (sequentially) exploits spatial locality and is therefore cheaper than loading data vertically.
Additionally, temporal reuse can only happen when other memory accesses do not displace reusable data from the cache.

\citeauthor{lam1991cache} proposed a method to model cache interference.
In the simplest case where all data used is cached in different locations (and therefore no data is evicted), the number of misses per variable $v$ is described by $D(v)/R(v)$, where $D(v)$ is the total number of references (loads) of $v$ and $R(v)$ is the reuse factor.
However, with interference misses when data gets displaced from cache, the total number of misses for $v$ is
\[
    D(v)\left(\frac{1}{R(v)}+\frac{R(v)-1}{R(v)}M(v)\right)
\]

With missrate being
\[
    M(v) = 1 \left(1 - S(v)\right) \prod_{u\in V - \{v\}}\left(1 - F(u)\right)
\]
$F(u)$ is defined as the fraction of cache used by variable $u$, and self interference $S(v)$ is defined to be the fraction of accesses that map to non-unique location in the cache.

\TODO{Illustrations for interference}


\subsection{CPU vs GPU based multi-threading}
\TODO{Aka, the limits of GPUs, segway to introduce Accelerate}

\subsection{Accelerate}
\TODO{Generalize to Array DSLs}
\label{sec:accelerate}
Accelerate is an embedded purely functional array language in Haskell \cite{chakravarty2011accelerating}.
Accelerate has a frontend containing the embedded language, and the backend which handles code generation and execution.
The frontend handles general optimizations such as sharing recovery and array fusion \cite{mcdonell2013optimising,balen2020optimal}.
Further hardware specific optimization is handled on the various backends.
There are two LLVM \cite{llvm} backends provided: one that targets multicore CPUs \texttt{accelerate-llvm-native} and one that targets Nvidia GPUs \texttt{accelerate-llvm-ptx}.
In both backends we compile Accelerate code to LLVM IR.
When we want to run Accelerate on a GPU, LLVM will handle the compilation from LLVM IR to PTX, the instructions set for Nvidia's CUDA programming environment \cite{mcdonell2015type, llvm, nvidia2021cudadocs}.
The GPU backend implements a series of skeletons which implement primitive operations such as stencils, generate, permute, and scan.
These skeletons define how a program should be compiled and is the part where a custom thread scheduler can be implemented.
Further customizations to the scheduler can be done on the executing side of the backend as it controls how kernels are launched.

\subsection{Commonly Applicable Cache Improvements}

\subsubsection{Optimizations of Blocked Algorithms}
\label{sec:optimization_blocked}
\citeauthor{lam1991cache} expands on the well known idea of working on blocks instead of entire rows or columns.

If all data fits onto cache without eviction, the misses that occur are \textit{intrinsic misses}.
In the real world however, data can be evicted by other memory accesses.
This interference of reuse is categorized between two cases: \textit{cross interference} and \textit{self interference}.
Cross interference assumes the location of data in memory is unrelated to the location in cache, and instead is measured by probablity that the reuse falls within the footprint of the variable.
Self interference extends this by taking the cache locations of variables into account, which can happen when the data for a single iteration no longer fits in cache.
\cite{lam1991cache}

\subsubsection{CTA Clustering}
\TODO{Write}

\subsubsection{PAVER}
\TODO{wRITE}

\section{Analysis of Existing Approaches}


\subsection{Spatial Temporal Analysis}
The memory accesses of algorithms can be plotted in a spatial-temporal diagram, with the address space on the spatial axis and order of access on the temporal axis.
Since only the thread order can be manipulated, elements on the temporal can be condensced by grouping them by thread.
Additionally, a cache simulation can annotate this diagram with the cache level (L1, L2, RAM) of each memory address.

\TODO{Example diagrams, cache simulation}

\subsection{Stencil Operations}

Stencil operation produces an N-dimensional array from a same sized input.
For each element it reads a fixed sized neighborhood and writes a single element.
Stencils are used for image operations (edge detection, filters, noise reduction), but can also find their use in other fields such as solving partial differentiation\cite{roth1997compilingstencils} and cellular automata.

\subsubsection{Naive}
\label{sec:stencil_naive}
The naive implementation iterates over each of the outputs linearly, horizontally first.
The temporal linearity translates to parallelism on GPUs where multiple threads work concurrently on each element of the output array.

While the naive implementation of stencil operations works well enough when enough rows of the input fit in the cache, it begins to fall in performance on larger inputs.

\TODO{Split figure \ref{fig:st_stencil} up into their respective parts}

% fig:st_stencil
\begin{figure}[!hb]
    \centering
    \subfloat[The spatial temporal diagram of a 7x7 stencil with linear ordering.]{
        \begin{tikzpicture}
            \node (image) at (0,0) {\includegraphics[width=0.4\textwidth]{st_diagrams/stencil7x7_linear.png}};
            \draw [->] (image.south west) -- ++(2,0) node[right]{\footnotesize\textit{Time}};
            \draw [->] (image.south west) -- ++(0,2) node[rotate=90, above]{\footnotesize\textit{Address}};
        \end{tikzpicture}
        \label{fig:st_stencil_naive}
    }
    \qquad
    \subfloat[The spatial temporal diagram of a 7x7 stencil with a column of size 4 ordering.]{
        \centering
        \begin{tikzpicture}
            \node (image) at (0,0) {\includegraphics[width=0.4\textwidth]{st_diagrams/stencil7x7_column_4.png}};
            \draw [->] (image.south west) -- ++(2,0) node[right]{\footnotesize\textit{Time}};
            \draw [->] (image.south west) -- ++(0,2) node[rotate=90, above]{\footnotesize\textit{Address}};
        \end{tikzpicture}
        \label{fig:st_stencil_column}
    }

    \caption{
        The spatial temporal diagrams of a 7x7 stencil. The vertical axis describes the location in a 2D array which is mapped to 1D address space. The horizontal axis describes time. \fsquare{red} are addresses of cache lines that are brought into cache. \fsquare{black} are addresses being accessed. \fsquare{gray} are addresses in cache.
    }
    \label{fig:st_stencil}
\end{figure}

% fig:stencil_naive_loading_pattern
\begin{figure}[!hb]
    \centering
    
    \subfloat[Ideal cache size, only minimal amount of loading is required.]{
        \centering
        \makebox[0.4\columnwidth][c]{
            \begin{tikzpicture}[scale=0.25, decoration = {
                markings, mark = between positions 0.3cm and 0.95 step 0.25cm with {\arrow{stealth}}
            }]
                \fill[gray!50]  ( 6, 6) rectangle +(10, 1);
                \fill[gray!50]  ( 0, 2) rectangle +(16, 4);
                \fill[gray!50]  ( 0, 1) rectangle +(10, 1);
                \fill[gray!100] (10, 1) rectangle +( 1, 1);

                \draw[step=1,gray!25] (0, 0) grid (16, 8);
                \draw[black, postaction = decorate] (8.5, 4.5) -- +(5, 0);
                \draw[black, dashed] (13.5, 4.5) -- (2.5, 3.5);
                \draw[black, postaction = decorate] (2.5, 3.5) -- +(6, 0);
                \draw[red] (6,1) rectangle +(5, 5);
            \end{tikzpicture}
        }
        \label{fig:stencil_naive_loading_pattern_ideal}
    }
    \qquad
    \subfloat[Cache too small, data gets evicted before any potential reuse.]{
        \centering
        \makebox[0.4\columnwidth][c]{
            \begin{tikzpicture}[scale=0.25, decoration = {
                markings, mark = between positions 0.3cm and 0.95 step 0.25cm with {\arrow{stealth}}
            }]
                \fill[gray!50]  (10, 2) rectangle +( 6, 5);
                \fill[gray!50]  ( 0, 1) rectangle +( 8, 5);
                \fill[gray!100] ( 8, 1) rectangle +( 1, 5);

                \draw[step=1,gray!25] (0, 0) grid (16, 8);
                \draw[black, postaction = decorate] (12.5, 4.5) -- +(1, 0);
                \draw[black, dashed] (13.5, 4.5) -- (2.5, 3.5);
                \draw[black, postaction = decorate] (2.5, 3.5) -- +(4, 0);
                \draw[red] (4,1) rectangle +(5, 5);
            \end{tikzpicture}
        }
    }
    \caption{
        \TODO{write this}
    }
    \label{fig:stencil_naive_loading_pattern}
\end{figure}

The minimum amount of cachelines cached $M_l$, and therefore needed, of cache line size of $L$ elements to stay in the ideal case is correlated by the stencil size and width of the input array:

\TODO{Maths}

\begin{equation}
    M_l \geq \ceil{\frac{I_w + S_w}{L}} S_h \label{eq:stencil_naive_optimal_memory}
\end{equation}

While in most cases \TODO{examples} this enough to fully exploit L2 caches, this can be unoptimal in regard to the L1 cache.
\TODO{Example}
The amount of cache lines needed to be feteched from memory $F$ is bound by the worse case (eq. \ref{eq:stencil_naive_optimal_memory} is not satisfied) where we consistently evict data from cache before we can reuse:

\[
    F \leq \ceil{\frac{I_w + S_w}{L}} I_h S_h
\]

If equation \ref{eq:stencil_naive_optimal_memory} is satisfied, the amount of fetches $F$ is no longer depedent on the stencil height $S_h$

\[
    F = \ceil{\frac{I_w + S_w}{L}} I_h
\]

\TODO{GPU threading is different from CPU stuff yada yada}
With multiple threads active, even more data is required to be kept in cache for optimal usuage.
In the best case, all threads are cohered with overlapping accesses, and in the worst case, threads will be spread out more with less overlapping accesses.
Threads in GPUs are grouped by warps, threads contained within are always cohered, and therefore a guarantee for overlapping accesses.
Therefore, only when multiple warps are executed on the same SM, divergence in accesses can occur.
A single warp of 32 threads uses $\ceil{\frac{32 + S_w}{l}} S_h$ cache lines when the threads cover a single rows. 
When the warp is split between 2 rows, the cache needs to be slightly bigger: $\ceil{\frac{32 + 2 S_w}{l}} S_h$.

Ideally, the whole input array would fit on the cache, but a sufficiently large input (i.e. a $2048\times2048$ 32-bit floating point array uses 16 MiB) will not fit on the L2 caches of modern GPUs ($\approx6$MiB of L2 data cache, Volta V100) and cache misses are unavoidable.
Even if data would fit on the L2 cache, there would still be potential cache misses at the L1 cache (128 KiB, Volta V100).

\TODO{More figures like in fig. \ref{fig:stencil_naive_loading_pattern}, but for GPU/multithreading}

\vspace{2cm}

Using the model described in section \ref{sec:cache_gpu} can be used to estimate the cache misses of the naive implementation and the model parameters are summarized in table \ref{tab:stencil_naive_model}.
Calculating the reuse for an iteration of $s_y$, $i_x$, and $i_y$ is fairly trivial.

\TODO{These are the calculations adapted from Lam et al. 1991. I'm not really sure if I should do this. They feel not very ellegant, and more like a black box system\dots}
$s_x$ is ommited due to it only loading a singular value during one loop and has therefore no reuse.
During a single step of $s_y$ an entire row of all $S_w$ elements from the stencil has been loaded.
A single step on $i_x$ processes the output of a singular element, which means an entire stencil is read.


\begin{table}[H]
    \centering
    \begin{tabular}{|c||c|c|c|c||c|c|c|c|}
        \hline
        Array & \multicolumn{4}{c||}{Reuse} & \multicolumn{4}{c|}{Self-Interference}
        \\ \hline
        & $i_x$ & $i_y$ & $s_x$ & $s_y$
        & $i_x$ & $i_y$ & $s_x$ & $s_y$
        \\ \hline
        X & $(S_w - 1)S_h$ & $(S_h - 1) S_w I_w$ & - & $S_w$
        & &  & 0 & 0

        \\ \hhline{*{9}{=}}
        Array & \multicolumn{4}{c||}{Footprint} &  \multicolumn{4}{c|}{References}
        \\ \hline
        & $i_x$ & $i_y$ & $s_x$ & $s_y$ & \multicolumn{4}{c|}{}
        \\ \hline
        X & $S_wS_h/C$ & $I_wS_h/C$ & $1/C$ & $S_w/C$ &
        \multicolumn{4}{c|}{$I_w I_h S_w S_h$}
        \\ \hline
    \end{tabular}

    \caption{
        The reuse, self-interference, and footprint of the naive implementation of stencils during a single step of iterating on $i_x$, $i_y$, $s_x$, and $s_y$.
        \TODO{Calculate self interference}
    }
    \label{tab:stencil_naive_model}
\end{table}

\subsubsection{Tiling}
\label{sec:stencil_tiled}
A common used optimization is by dividing the work into works as described in \ref{sec:optimization_blocked}.
\TODO{blablablabla}

The minimum required cache for optimal tiling is depedent on \TODO{\dots} tiling size $t$

\begin{equation}
    M_l \geq \ceil{\frac{t + S_w}{L}} S_h \label{eq:stencil_tiling_memory}
\end{equation}

The largest possible tiling size $t$ is derived by inverting equation \ref{eq:stencil_tiling_memory}

\begin{equation}
    t \leq \frac{L M_l}{S_h} - S_w
\end{equation}

Since equation \ref{eq:stencil_tiling_memory} can always be satisfied by adjusting $t$, we can have a lower upper bound on the amount of cache line fetches $F$:

\begin{equation}
    F \leq \ceil{\frac{I_w}{t}} \ceil{\frac{t + S_w}{L}} \ceil{\frac{I_h}{t}} (t + S_h)
    \label{eq:stencil_tiling_fetches}
\end{equation}

\TODO{Maybe remove this. Probably just keep it, so we can plot our expected number of cache line fetches}
We can define the number cache lines fetched in terms of the available cache by substituting equation \ref{eq:stencil_tiling_memory} into equation \ref{eq:stencil_tiling_fetches}:

\begin{equation}
    F \leq \ceil{\frac{I_w}{\frac{L M_l}{S_h} - S_w}} \ceil{\frac{\frac{L M_l}{S_h} - S_w + S_w}{L}} \ceil{\frac{I_h}{\frac{L M_l}{S_h} - S_w}} \left(\frac{L M_l}{S_h} - S_w + S_h\right)
\end{equation}

\begin{equation}
    F \leq \frac{I_w I_h M_l}{(\frac{L M_l}{S_h} - S_w)^2 S_h} (\frac{L M_l}{S_h} - S_w + S_h)
\end{equation}
\TODO{simplify this further by relaxation perhaps?}


\TODO{GPU/multithreading notes}

\subsection{Matrix Multiplication}

\subsubsection{Naive}

\TODO{Split figure \ref{fig:st_matrix} up into their parts for better flow}
% fig:st_matrix
\begin{figure}[!h]
    \centering
    \subfloat[The spatial temporal diagram of matrix multiplication with linear ordering.]{
        \begin{tikzpicture}[spy using outlines={rectangle, magnification=4,connect spies}]
            \node (image) at (0,0) {\includegraphics[width=0.25\textwidth]{st_diagrams/matrix_linear.png}};
            \draw [->] (image.south west) -- ++(2,0) node[right]{\footnotesize\textit{Time}};
            \draw [->] (image.south west) -- ++(0,8.5) node[rotate=90, above left]{\footnotesize\textit{Memory Addresses}};

            \coordinate (p) at (0, 1);
            \coordinate (v) at (3.3, 2);
            \spy[width=2.4cm,height=4cm] on (p) in node [fill=white] at (v);
        \end{tikzpicture}
        \label{fig:st_matrix_naive}
    }
    \qquad
    \subfloat[The spatial temporal diagram of a matrix multiplication with a column of size 4 ordering.]{
        \centering
        \begin{tikzpicture}[spy using outlines={rectangle, magnification=4,connect spies}]
            \node (image) at (0,0) {\includegraphics[width=0.25\textwidth]{st_diagrams/matrix_column_4.png}};
            \draw [->] (image.south west) -- ++(2,0) node[right]{\footnotesize\textit{Time}};
            \draw [->] (image.south west) -- ++(0,8.5) node[rotate=90, above left]{\footnotesize\textit{Memory Addresses}};

            \coordinate (p) at (0, 1);
            \coordinate (v) at (3.3, 2);
            \spy[width=2.4cm,height=4cm] on (p) in node [fill=white] at (v);
        \end{tikzpicture}
        \label{fig:st_matrix_column}
    }

    \caption{
        The spatial temporal diagrams of a matrix multiplication. The vertical axis describes the location in a 2D array which is mapped to 1D address space. The horizontal axis describes time. \fsquare{red} are addresses of cache lines that are brought into cache. \fsquare{black} are addresses being accessed. \fsquare{gray} are addresses in cache.
    }
    \label{fig:st_matrix}
\end{figure}

\subsubsection{Tiling}

\section{Column Based Iteration}

\TODO{Consider splitting in two chapters if they get too big}

While tiling itself is a well-known optimization technique, we most consider the parts of why it works.

\TODO{\dots}

In a sense, grouping columns is similar to striding clusters of data, except in the case when a row of work can't be perfectly strided.
When the width of a multidimensional array is not a multiple of the stride, the loads will not align column wise (\TODO{figure}).
To work around this, in the column approach we allow the last column to be narrower.

\begin{figure}[!hb]
    \centering
    
    \subfloat[Striding.]{
        \centering
        \makebox[0.4\columnwidth][c]{
            \begin{tikzpicture}[scale=0.25, decoration = {
                markings, mark = between positions 0.3cm and 0.95 step 0.25cm with {\arrow{stealth}}
            }]
                \fill[gray]    (0, 3) rectangle +(4, 1);
                \fill[gray!50] (4, 3) rectangle +(4, 1);
                \fill[gray]    (8, 3) rectangle +(3, 1);
                \draw[step=1,gray!25] (0, 0) grid (11, 4);
            \end{tikzpicture}
        }
        \label{fig:striding_misalignment}
    }
    \qquad
    \subfloat[Column]{
        \centering
        \makebox[0.4\columnwidth][c]{
            \begin{tikzpicture}[scale=0.25, decoration = {
                markings, mark = between positions 0.3cm and 0.95 step 0.25cm with {\arrow{stealth}}
            }]
                \draw[step=1,gray!25] (0, 0) grid (16, 8);
            \end{tikzpicture}
        }
        \label{fig:column_alignment}
    }
    \caption{
        \TODO{write this}
    }
    \label{fig:column_vs_striding}
\end{figure}

\TODO{implementation details}

We can construct the index mapping $i \mapsto j$. First, we calculate the offset for the starting index of the column we need to map to

\begin{align}
    o  &= \floor{\frac{i}{I_h w}} w
    \\ \intertext{Then, modify the width value such that the last column does not exceed the input width}
    w' &= \begin{cases}
        ((I_w - 1) \bmod w) + 1& \text{if last column}
        \\
        w & \text{otherwise}
    \end{cases}
    \\ \intertext{And take $i'$ as the index within a column.}
    i' &= i \bmod I_h w'
    \\ \intertext{So that we can calculate $j$}
    j  &= (i' \bmod w')' + \floor{\frac{i'}{w'}} I_w + o
\end{align}

\TODO{should i add a figure showing the relation between the equation and the column pattern?}

\TODO{What are the effects of GPU warps (32 threads) with this schedule?}

\subsection{Zigzagging variation}
\TODO{If we make columns fit for L2 cache, zigzagging may improve locality for lower level cache (L1) when columns sizes aren't a nice power of 2 (or)}

\subsection{Implementation in Accelerate}

\TODO{AAAAAAAAAAAA}

\subsection{Stencil Operation}

The naive stencil operations had problems of chasing the cache on sufficiently large input sizes which tiling does resolve (section \ref{sec:stencil_tiled}).
Consider what the tiling implies: we split the work on all axis, to improve locality.
However, all tiling except horizontal is unnecessary \TODO{bold claim, needs support} and may cause even more discontunuity in favourable access pattern.

Let us first consider the single threaded case of column based itteration: by controlling the column width we can force the ideal scenario of the naive stencil implementation (figure \ref{fig:stencil_naive_loading_pattern_ideal}) to occur, similarly to tiling.
The column based approach is similar to tiling, but also allows the ideal memory access pattern to continue accross tiles vertically.
In GPUs this translates to less cohesive threads as threadblocks get assigned in a round-robin fashion to SMs, eliminating posibilities of L1 cache reuse.

\begin{figure}[!hb]
    \centering
    
    \subfloat[Grouping by column only incurs a heavy load every time a new column is started.]{
        \centering
        \makebox[0.4\columnwidth][c]{
            \begin{tikzpicture}[scale=0.25, decoration = {
                markings, mark = between positions 0.3cm and 0.95 step 0.25cm with {\arrow{stealth}}
            }]
                \fill[gray!50]  (-1, -1) rectangle +(6, 4);
                \fill[gray!15] (3, 6) rectangle +(3, 3);

                \draw[step=1,gray!25] (-1, -1) grid (9, 9);

                \draw[gray, postaction = decorate] (0.5, 7.5) -- +(3, 0);
                \draw[gray, dashed] (3.5, 7.5) -- +(-3, -1);
                \draw[gray, postaction = decorate] (0.5, 6.5) -- +(3, 0);
                \draw[gray, dashed] (3.5, 6.5) -- +(-3, -1);
                \draw[gray, postaction = decorate] (0.5, 5.5) -- +(3, 0);
                \draw[gray, dashed] (3.5, 5.5) -- +(-3, -1);
                \draw[gray, postaction = decorate] (0.5, 4.5) -- +(3, 0);
                \draw[gray, dashed] (3.5, 4.5) -- +(-3, -1);
                \draw[gray, postaction = decorate] (0.5, 3.5) -- +(3, 0);
                \draw[gray, dashed] (3.5, 3.5) -- +(-3, -1);
                \draw[gray, postaction = decorate] (0.5, 2.5) -- +(3, 0);
                \draw[gray, dashed] (3.5, 2.5) -- +(-3, -1);
                \draw[black, postaction = decorate] (0.5, 1.5) -- +(3, 0);
                \draw[black, dashed] (3.5, 1.5) -- +(-3, -1);
                \draw[black, postaction = decorate] (0.5, 0.5) -- +(3, 0);

                \draw[black, dashed] (3.5, 0.5) -- +(1, 7);
                
                \draw[gray, postaction = decorate] (4.5, 7.5) -- +(3, 0);
                \draw[gray, dashed] (7.5, 7.5) -- +(-3, -1);
                \draw[gray, postaction = decorate] (4.5, 6.5) -- +(3, 0);
                \draw[gray, dashed] (7.5, 6.5) -- +(-3, -1);
                \draw[gray, postaction = decorate] (4.5, 5.5) -- +(3, 0);
                \draw[gray, dashed] (7.5, 5.5) -- +(-3, -1);
                \draw[gray, postaction = decorate] (4.5, 4.5) -- +(3, 0);
                \draw[gray, dashed] (7.5, 4.5) -- +(-3, -1);
                \draw[gray, postaction = decorate] (4.5, 3.5) -- +(3, 0);
                \draw[gray, dashed] (7.5, 3.5) -- +(-3, -1);
                \draw[gray, postaction = decorate] (4.5, 2.5) -- +(3, 0);
                \draw[gray, dashed] (7.5, 2.5) -- +(-3, -1);
                \draw[gray, postaction = decorate] (4.5, 1.5) -- +(3, 0);
                \draw[gray, dashed] (7.5, 1.5) -- +(-3, -1);
                \draw[gray, postaction = decorate] (4.5, 0.5) -- +(3, 0);
                \draw[red] (3,6) rectangle +(3, 3);
            \end{tikzpicture}
        }
    }
    \label{fig:}
    \qquad
    \subfloat[Grouping by column and row (tiling) also incurs a heavy load when starting a new row, and due to having more rows also has more column starts.]{
        \centering
        \makebox[0.4\columnwidth][c]{
            \begin{tikzpicture}[scale=0.25, decoration = {
                markings, mark = between positions 0.3cm and 0.95 step 0.25cm with {\arrow{stealth}}
            }]
            \fill[gray!50] (3, 3) rectangle +(6, 4);
            \fill[gray!15] (-1, 2) rectangle +(3, 3);

            \draw[step=1,gray!25] (-1, -1) grid (9, 9);

            \draw[gray, postaction = decorate] (0.5, 7.5) -- +(3, 0);
            \draw[gray, dashed] (3.5, 7.5) -- +(-3, -1);
            \draw[gray, postaction = decorate] (0.5, 6.5) -- +(3, 0);
            \draw[gray, dashed] (3.5, 6.5) -- +(-3, -1);
            \draw[gray, postaction = decorate] (0.5, 5.5) -- +(3, 0);
            \draw[gray, dashed] (3.5, 5.5) -- +(-3, -1);
            \draw[gray, postaction = decorate] (0.5, 4.5) -- +(3, 0);
            \draw[gray, dashed] (3.5, 4.5) -- +(1, 3);
            \draw[gray, postaction = decorate] (4.5, 7.5) -- +(3, 0);
            \draw[gray, dashed] (7.5, 7.5) -- +(-3, -1);
            \draw[gray, postaction = decorate] (4.5, 6.5) -- +(3, 0);
            \draw[gray, dashed] (7.5, 6.5) -- +(-3, -1);
            \draw[black, postaction = decorate] (4.5, 5.5) -- +(3, 0);
            \draw[black, dashed] (7.5, 5.5) -- +(-3, -1);
            \draw[black, postaction = decorate] (4.5, 4.5) -- +(3, 0);
            
            \draw[black, dashed] (7.5, 4.5) -- +(-7, -1);

            \draw[gray, postaction = decorate] (0.5, 3.5) -- +(3, 0);
            \draw[gray, dashed] (3.5, 3.5) -- +(-3, -1);
            \draw[gray, postaction = decorate] (0.5, 2.5) -- +(3, 0);
            \draw[gray, dashed] (3.5, 2.5) -- +(-3, -1);
            \draw[gray, postaction = decorate] (0.5, 1.5) -- +(3, 0);
            \draw[gray, dashed] (3.5, 1.5) -- +(-3, -1);
            \draw[gray, postaction = decorate] (0.5, 0.5) -- +(3, 0);
            \draw[gray, dashed] (3.5, 0.5) -- +(1, 3);
            \draw[gray, postaction = decorate] (4.5, 3.5) -- +(3, 0);
            \draw[gray, dashed] (7.5, 3.5) -- +(-3, -1);
            \draw[gray, postaction = decorate] (4.5, 2.5) -- +(3, 0);
            \draw[gray, dashed] (7.5, 2.5) -- +(-3, -1);
            \draw[gray, postaction = decorate] (4.5, 1.5) -- +(3, 0);
            \draw[gray, dashed] (7.5, 1.5) -- +(-3, -1);
            \draw[gray, postaction = decorate] (4.5, 0.5) -- +(3, 0);
            \draw[red] (-1,2) rectangle +(3, 3);
            \end{tikzpicture}
        }
    }
    \caption{
        Visualisation of cache bottlenecks for both column based and tiling approaches. 
        \fsquare{red} Memory required by the next task.
        \fsquare{black} Tasks that still have their memory in cache.
        \fsquare{gray!50} Memory in cache.
        \fsquare{gray} Previous and upcoming tasks.
    }
    \label{fig:stencil_column_vs_tiling_pattern}
\end{figure}

\TODO{WRITEAAAAAAAAAAAAAA}

\vspace{4cm}

The size of the cache needed $M_{size}$ can be approximated given the unit byte size $u$, column width $c$, stencil width $s_{w}$ and height $s_{h}$, and the number of active threads $t$. The required memory is independent of the size of the input data.

\[
    M_{size} = u \left(c + s_w\right) \left(s_h + \ceil{\frac{t}{c}}\right)
\]

Solving for $c$ gives an unneedly complex solution, so instead we approximate $M_{size}$ using the asymptote.

\[
    M_{size} = u \left(t + s_h \left(c + s_w\right)\right)
\]

Solving for column width $c$ gives 

\[
    c = \frac{M_{size} - u (s_h s_w + t)}{s_h u}
\]

\TODO{amount cache line fetches text here}
The amount of fetches is similar to equation \ref{eq:stencil_tiling_fetches}, however, since we do not divide the work horizontally anymore, the amount of cache line fetches is slightly less:

\begin{equation}
    F \leq \ceil{\frac{I_w}{c}}\ceil{\frac{c + S_w}{L}} (I_h + S_h)
    \label{eq:stencil_column_fetches}
\end{equation}

\TODO{Is a difference useful?}
\begin{equation}
    \Delta F \approx  
\end{equation}

\TODO{Calculate column parameters, spatial temporal diagrams}

\subsection{Matrix Multiplication}


A matrix multiplication of $A \cdot B = C$ consist out of three parts: reading horizontally from matrix $A$, reading vertically from matrix $B$, and writing to matrix $C$.
Since writing the output does not block any further instructions, it can be ignored.
Reading horizontally is significantly cheaper due to cache locality.

\[
    M_{size} = u \left(h c + w \ceil{\frac{t}{c}}\right)
\]

Similarly to stencil operations, approximate $M_{size}$ using the asymptote

\[
    M_{size} = u h c 
\]

Then solving for $c$

\[
    c = \frac{M_{size}}{u h}
\]

\TODO{Calculate column parameters, spatial temporal diagrams}

\section{Results}
\TODO{Benchmark results, Nvidia profiler statistics}
\subsection{Execution Times}
\subsection{Cache Hitrate}
\subsection{Hardware Utilization}

\section{Conclusion}

\bibliographystyle{unsrtnat}
\bibliography{bibliography}
\end{document}
