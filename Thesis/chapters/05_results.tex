The optimization described in chapter \ref{chap:cbi} has been implemented as both a native CUDA program and a modification to the CUDA Accelerate backend.
Both implementations are benchmarking stencil operations and matrix multiplication on a Nvidia RTX 2080 Super GPU with a AMD Ryzen 5800x3D as CPU.

\section{CUDA Benchmarks}

Two simple problems have been implemented as a simple CUDA program: a $9 \times 9$ stencil operation on a $4096 \times 4096$ input, and a matrix multiplication on two $1024 \times 1024$ arrays of 32-bit floating point numbers.
We execute the program naively, with the no remapping, and with our optimization: column based iteration.
The kernel is analyzed with Nvidia profiler (\texttt{nvprof}) and Nvidia Nsight Compute CLI (\texttt{ncu}).
Contrary to our expectations, the kernel execution time seems to be not correlated to our estimated column width at all, instead prefering lower multiple of 32 widths regardless of block size (figure \ref{fig:cuda_kernel_times}).


\begin{figure}[H]
    \centering
    \makebox[\textwidth][c] {
        \subfloat[The execution time in milliseconds of a $9 \times 9$ stenciling kernel on a $4096 \times 4096$ input array with various column sizes and block sizes (prefixed with \textit{b}).]{
            \centering
            \pgfplotstabletypeset[col sep=comma,
                /color cells/max=1.3,
                /color cells/min=4.2,
                /color cells/textcolor=black,
                columns/b32/.style={color cells},
                columns/b64/.style={color cells},
                columns/b128/.style={color cells},
                columns/b256/.style={color cells},
                columns/b512/.style={color cells},
                columns/b1024/.style={color cells},
                columns/bench/.style={string type},
                /pgfplots/colormap name=viridis-light]{kernel_time_stencil.csv}
        }
        \label{fig:cuda_kernel_time_stencil}
        \qquad
        \subfloat[The execution time in milliseconds of matrix multiplications on two $1024 \times 1024$ input arrays with various column sizes and block sizes (prefixed with \textit{b}).]{
            \centering
            \pgfplotstabletypeset[col sep=comma,
                /color cells/max=1.7,
                /color cells/min=4.2,
                /color cells/textcolor=black,
                columns/b32/.style={color cells},
                columns/b64/.style={color cells},
                columns/b128/.style={color cells},
                columns/b256/.style={color cells},
                columns/b512/.style={color cells},
                columns/b1024/.style={color cells},
                columns/bench/.style={string type},
                /pgfplots/colormap name=viridis-light]{kernel_time_matrix.csv}
        }
    }
    \label{fig:cuda_kernel_time_matrix}
    \caption{
        The execution time of stenciling operations and matrix multiplication on a power-of-two input. Lower is better.
    }
    \label{fig:cuda_kernel_times}
\end{figure}

The factor of 32 seems to be independent of input size and upholds during non power of 2 inputs (figure \ref{fig:cuda_kernel_time_stencil_no_pow2}).
Changing the data type to half precision (16-bit) floating points had effect on the overal improvements.
Using 64-bit double precision floating points made the kernel compute bound instead of memory bound and therefore no tangible difference between the naive implementation and any column remapping is observed.
This exeggerates our earlier assumption (section \ref{sec:implementation_theory}) that the 32 thread wide warps might play a role.

\begin{figure}[H]
    \centering
    \pgfplotstabletypeset[col sep=comma,
        /color cells/max=1.25,
        /color cells/min=4.2,
        /color cells/textcolor=black,
        columns/b32/.style={color cells},
        columns/b64/.style={color cells},
        columns/b128/.style={color cells},
        columns/b256/.style={color cells},
        columns/b512/.style={color cells},
        columns/b1024/.style={color cells},
        columns/bench/.style={string type},
        /pgfplots/colormap name=viridis-light]{kernel_time_stencil_4037.csv}
    \caption{The execution time in milliseconds of a $9 \times 9$ stenciling kernel on a $4037 \times 4037$ input array with various column sizes and block sizes (prefixed with \textit{b}). Even on non power of two's, column widths of multiples of 32 produce more optimal results.}
    \label{fig:cuda_kernel_time_stencil_no_pow2}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            xlabel = Input Size,
            ylabel = Execution time compared to naive,
            xmode = log,
            ymajorgrids,
            %log ticks with fixed point,
            log basis x={2},
            xmin=128,
            xmax=8192,
            xtick={128,256,512,1024,2048,4096,8192},
            colormap name=viridis,
            cycle list={[colors of colormap={0, 150,..., 1000}]},
            every axis plot/.append style={thick},
            legend pos=north west,
            height=10cm,
        ]
            \addplot[mark=none, black, samples=2, domain=128:8192] {1};
            \addlegendentry{Naive}
            \addlegendimage{empty legend}
            \addlegendentry[yshift=10pt]{\textbf{Column}}
            \pgfplotsinvokeforeach{32, 64, 128, 256, 512, 1024, 2048} {
                \addplot table[
                    x = size,
                    y expr  = \thisrow{c#1}/\thisrow{c0},
                    col sep = comma,
                ]{kernel_time_vs_input_stencil.csv};
                \addlegendentry{#1}
            }
        \end{axis}
    \end{tikzpicture}
    \caption{
        The relative kernel execution time of various column widths compared to the naive implementation for a $9 \times 9$ stencil operation on an $N \times N$ matrix. Lower is better.
    }
\end{figure}

Execution time for matrix multiplication with CBI compared to naive only appears to improve after a certain input size ($2^8$) and has diminishing returns after ($2^10$), see figure \ref{fig:mm_naive_vs_cbi}.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            xlabel = Input Size,
            ylabel = Execution time compared to naive,
            xmode = log,
            ymajorgrids,
            %log ticks with fixed point,
            log basis x={2},
            xmin=128,
            xmax=2048,
            xtick={128,256,512,1024,2048},
            colormap name=viridis,
            cycle list={[colors of colormap={0, 150,..., 1000}]},
            every axis plot/.append style={thick},
            legend pos=south west,
            height=10cm,
        ]
            \addplot[mark=none, black, samples=2, domain=128:8192] {1};
            \addlegendentry{Naive}
            \addlegendimage{empty legend}
            \addlegendentry[yshift=10pt]{\textbf{Column}}
            \pgfplotsinvokeforeach{32, 64, 128, 256, 512, 1024, 2048} {
                \addplot table[
                    x = size,
                    y expr  = \thisrow{c#1}/\thisrow{c0},
                    col sep = comma,
                ]{kernel_time_vs_input_matrix.csv};
                \addlegendentry{#1}
            }
        \end{axis}
    \end{tikzpicture}
    \caption{
        The relative kernel execution time of various column widths compared to the naive implementation for the matrix multiplication of two $N \times N$ matrices. Lower is better.
    }
    \label{fig:mm_naive_vs_cbi}
\end{figure}

\section{Hardware Utilization}
On the $1024 \times 1024$ matrix multiplication, running a 32 wide column patterns resulted in an improvement to the L1 cache hit rate from 19.49\% to 77.40\%.
This results in a drop in transferred data from L2 to L1 cache from 4.02 GB to 1.13 GB. 
The L2 hit rate dropped from 98.52\% in the naive case to 95.73\%.

In both the naive and column pattern, Nvidia Nsight Compute reported that the scheduler is stalling due to the L1 instruction queue for local and global memory operation being full.
However, the column pattern fix stalls on long scoreboard, meaning instruction could not be executed out of order due to a depedency on an unfinished L1TEX (local, global, surface, tex) operation.

\section{Accelerate Benchmarks}
\TODO{I don't have access to this data right now oops}
