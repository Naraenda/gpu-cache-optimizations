\section{GPU Architecture}
The GPU backend of Accelerate only works with CUDA capable devices (see section \ref{sec:accelerate}).
Therefore, we will mostly focus on the architecture of newer Nvidia GPUs.

\subsection{Hardware}
On modern Nvidia GPU architectures is composed of multiple GPU Processing Clusters (GPCs), Texture Processing Clusters (TPCs), Streaming Multiprocessors (SMs) and memory controllers.
The main point of interest are the streaming multiprocessors which handle the data processing.
The GPU uses a single instruction multiple threads (SIMT) execution model, and the scheduler in each SM dispatches instructions to multiple cores to the various specialized cores for each of the various execution pipelines (single and double precision computation, integer computation, etc.).

On the Turing, Volta and Ampera architectures each SM has 4 warp schedulers and the accompanying pipelines and therefore \citeauthor{jia2019dissecting} suggests that a threadblock should contain at least 128 threads due to SMs on Turing, Volta, and Ampere being split into four processing blocks so at least all 4 schedulures can be completely utilized. \cite{jia2019dissecting,nvidia2017volta,nvidia2018turing,nvidia2020ampere}.
The memory is structured in a multi-level hierarchy containing an L1 cache for each SM, a shared L2 cache for all SMs and multiple banks of DRAM \cite{nvidia2017volta,nvidia2020ampere} (figure \ref{fig:ampere_architecture}).
Data can be shared between threads in the same CTA, and therefore on the same SM, via the L1 cache and between all threads via L2 cache.

\begin{figure}[!hb]
    \centering
    \begin{tikzpicture} 
        \foreach \x in {0,1,2,3} {
            \draw (\x*2, 0) rectangle +(1.5, 0.5) node[pos=.5] {Memory};
            \draw[<->] (\x*2+0.75, 0) -- +(0, -0.5);
        }
        \draw (0, -1) rectangle +(7.5, 0.5) node[pos=.5] {L2 Cache};

        \draw[<->] (2, -1) -- +(0, -0.75);

        \draw (0, -1.5) rectangle +(3.25, -3.75);
        \draw (1, -1.75) rectangle +(2, -0.5) node[pos=.5] {L1 Cache};
        
        \pic [local bounding box=sm_warp] at (1, -2.5) {sm_warp};
        \node[left of=sm_warp, xshift=-4mm] {$4\times$};
        \node at (0.5, -1.75) {SM};
        \node[scale=2] at (5, -3.25)  {\dots\dots};
    \end{tikzpicture}
    \caption{
        An overview of the memory hierarchy of the Ampere architecture. The L1 Cache is local to the SM which executes can 4 warps simultaneously 
    }
    \label{fig:ampere_architecture}
\end{figure}

Aside from the L1 and L2 caches, GPUs also use a translation lookaside buffer (TLB) which caches recent translations from virtual address space to physical address space.
By working in virtual memory space, multiple programs can use memory ranges independently of other programs which may want to occupy the same range.

\subsection{GPU Caches}
\label{sec:cache_gpu}
To the programmer, the memory hierarchy is very simplified: there is the compute unit and the memory.
Caches are hidden from this model as on most architectures they are managed by hardware.

Memory can become a significant bottleneck due to the large amount of threads running concurrently and the amount of data each thread processes.
Caches are much faster than memory and are often on the same chip as the compute unit.
However, they are limited in size, and a large enough problem can cause cache trashing -- the premature eviction of data before any significant reuse \cite{dai2016model}.
To improve the efficiency of caches, caches asume spatial locality via cache lines.
A cache line is the smallest unit of data that a cache can hold, and fetching data from memory also brings extra nearby data with it.
The L1 cache on a Turing GPU (and most other modern architectures) uses 128 byte cachelines which plays well with the 32-thread warp size since executing a fetch for a single precision floating point takes $32 \times 4 = 128$ bytes which can fit in a single cache line.
Data shared between threads through the cache can happen in a read-after-write (RAW) or read-after-read (RAR) manner.
RAW has data dependency between tasks, for example in scan operations.
RAR has no data dependency and can be executed in any order \cite{tripathy2021paver}.

The L1 cache in older Nvidia GPU architectures (Maxwell, Pascal) uses the least recently used (LRU) eviction policy.
When caches become full, we need to remove data (a cache line) from the cache to allow newer data to be cached.
An LRU eviction policy evicts data that is the least recently used.
\citet{mei2016dissecting} presented a novel fine-grained pointer chasing (P-chase) microbenchmark to explore unknown GPU cache parameters.
P-chase defines an array of indices where each element points to the next index to fetch from the array, thus chasing the pointer.

\citet{jia2019dissecting} have shown that in Turing and Volta GPUs, the P-chase benchmark that is used to detect the LRU eviction policy presented by \citet{mei2016dissecting} fails to complete over the full L1 cache.
\citeauthor{jia2019dissecting} conclude that newer architectures (Turing, Volta) uses a non-LRU eviction policy \cite{jia2019dissecting, jia2018dissecting,mei2016dissecting}.
When the L1 cache in Turing and Volta GPU saturates, 4 consecutive cache lines are chosen randomly to be evicted.
This is in line with a new eviction policy mechanism introduced with Volta, where cache lines can be assigned a priority \cite{jia2019dissecting,nvidia2021cudadocs}.

On the Turing architecture \citeauthor{jia2019dissecting} has found with the P-chase benchmark that the memory access latancy for a L2 cache miss and TLB miss to be 616 cycles.
On a L1 cache hit (best case scenario) the latency is 32 cycles.
For a L2 cache hit this rises to a latency of 188 cycles.
With L2 miss but a TLB hit the latency becomes 296 cycles.

Modern Nvidia GPUs are able to handle various types cache operations and eviction hints.
By default, loads are cached at all levels (L2, L1) with an LRU policy.
This brings a problem with it: if data is writen to a cached value, we need to evict this cache line from all other L1 caches first, since that value is no longer up to date after our update.
As an example, it is also possible to only cache on L2, bypassing L1.
Another option is to hint cache streaming, where the loaded cache line will have an evict-first policy to prevent polution of the cache.
Similar operations exist for writing data to memory.
In both cases it is up to the compiler and programmer to exploit this for extra performance \cite{nvidia2021cudadocs}.


\begin{table}
    \centering
    \begin{tabular}{l l l|r r r r}
        & Architecture & &    Turing &      Volta & Pascal & Maxwell
        \\
        & GPU Board    & &        T4 &       V100 &   P100 &     M60
        \\
        & GPU Chip     & &     TU104 &      GV100 &  GP100 &   GM204
        \\
        & Year         & &      2018 &       2017 &   2016 &    2014
        \\
        \hline
        L1 Cache%
        & Size     & KiB &  32 or 64 & 32\dots128 &     24 &      24
        \\
        & Line size  & B &        32 &         32 &     32 &      32
        \\
        \hline
        L2 Cache%
        & Size     & KiB &      4096 &      6144 &   4096 &    2048
        \\
        & Line size  & B &        64 &        64 &     32 &      32
        \\
        \hline
        L1 TLB%
        & Coverage   & MiB &      32 &        32 &   \~32 &     \~2
        \\
        & Page entry & KiB &    2048 &    2048   &   2048 &     128
        \\
        \hline
        L2 TLB%
        & Coverage   & MiB &  \~8192 &  \~8192   & \~2048 &   \~128
        \\
        & Page entry & MiB &      32 &      32   &     32 &       2
    \end{tabular}
    \caption{
        Summary of the cache specification on various Nvidia GPUs and architectures. 
        Adapted from \citeauthor{jia2019dissecting}\cite{jia2019dissecting}.
    }
\end{table}

\subsection{Kernel Execution}
\label{sec:kernel_execution}
When working with CUDA the programmer defines a kernel.
This is normally done with CUDA C++, an extension on C++ programming language, but in our case Accelerate will handle the generation of kernels (section \ref{sec:accelerate}) \cite{nvidia2021cudadocs}.

Executions on a GPU are directed on both the host and device (GPU).
\textit{Kernels} define the functions that should be executed on the GPU.
The host side controls how these kernels should be executed, namely how the threads should be launched and executed.
Threads are grouped and defined on a 2 level hierarchy: threads are grouped together in \textit{cooperative thread arrays} (CTAs), also known as thread blocks, and multiple CTAs can be queued for the execution of a single kernel.
Both can be controlled upon executing a kernel: the amount of threads per CTA (threadblock size) and the total amount of CTAs (gridsize).
CTAs get assigned to SMs in an arbitrary manner.

When an SM executes a CTA, it splits the work into warps, a grouping of 32-threads.
On architecture before Volta, a single warp is executed in a single instruction, multiple threads fashion, where a single program counter is shared amongst the 32 threads.
With the volta architecture, independent thread scheduling allows full concurency between threads and the scheduler can group multiple threads into SIMT units.

SMs want to work on multiple warps since this can hide latency.
After dispatching an instruction to an execution unit, the schedulur looks for the next warp that can execute an instruction (for example, not waiting for data) 

\begin{figure}[!hb]
    \centering
    \begin{tikzpicture}
        \node at (1, 1.5) {threadblock size};
        \draw[decorate, thick, decoration={brace}] (0,1.1) --  (2,1.1);
        

        \node [rotate=90] at (-0.5, -1) {grid size};
        \draw[decorate, thick, decoration={brace}] (-0.1,-3.2) --  (-0.1,1);

        \pic at (0, 0) {threadblock};
        \pic at (0, -1.1) {threadblock};
        
        \node[scale=2, rotate=90] at (1, -1.6)  {\dots};
        \pic at (0, -3.2) {threadblock};
    \end{tikzpicture}
    \caption{
        GPU workloads are defined by the threadblock size and grid size.
    }
\end{figure}

\subsection{Performance of Access Patterns}
\label{sec:access_patterns}
\citeauthor{lam1991cache} and \citeauthor{meyer2003algorithms} describe two types of reference reuse \cite{lam1991cache, meyer2003algorithms}:
\begin{itemize}
    \item \textbf{Spatial reuse} occurs when accessing data from the same cache line, increasing spatial locality.
    \item \textbf{Temporal reuse} occurs when the same data is accessed at a later time, increasing temporal locality.
\end{itemize}
The reuse factor can be kept track of by counting the two types of reuse.
Loading data horizontally (sequentially) exploits spatial locality and is therefore cheaper than loading data vertically.
Additionally, temporal reuse can only happen when other memory accesses do not displace reusable data from the cache.

\citeauthor{lam1991cache} proposed a method to model cache interference.
In the simplest case where all data used is cached in different locations (and therefore no data is evicted), the number of misses per variable $v$ is described by $D(v)/R(v)$, where $D(v)$ is the total number of references (loads) of $v$ and $R(v)$ is the reuse factor.
However, with interference misses when data gets displaced from cache, the total number of misses for $v$ is
\[
    D(v)\left(\frac{1}{R(v)}+\frac{R(v)-1}{R(v)}M(v)\right)
\]

With missrate being
\[
    M(v) = 1 \left(1 - S(v)\right) \prod_{u\in V - \{v\}}\left(1 - F(u)\right)
\]
$F(u)$ is defined as the fraction of cache used by variable $u$, and self interference $S(v)$ is defined to be the fraction of accesses that map to non-unique location in the cache.
In the context of blocking, the largest block size where no self-interference occurs is called the critical blocking factor.
From \citeauthor{lam1991cache}'s observations with matrix multiplications, the total amount of cache misses gradually declines with larger block sizes until the critical blocking factor is reached.

\section{CPU vs GPU based multi-threading}
CPUs and GPUs differ in multiple ways.
GPUs consist out of many more cores compared to CPUs, however this also comes in a reduction in core complexity and capability.
Due to the CPUs being focused on single threads, it can exploit better branch prediction.
Another hardware specific optimization available is prefetching, where instructions and data can be loaded into cache that are likely to be needed in the near future.

While GPUs can exploit its SIMT nature for parallelization, CPUs may implement single instruction, multiple data (SIMD). For example, Streaming SIMD Extensions (SSE) and Advanced Vector Extensions (AVX, AVX2, and AVX-512) allows CPU to process multiple data with singular instructions.
SSE can handle four 32-bit single-precision floating point numbers while AVX and AVX2 handle eight.
AVX-512 can handle sixteen, but is not widely implemented.

Caches also share the same hierarchical structure as the cores, however this differs per manufacturer and architecture.
Often the cache L1 is local to each thread, and L2 local to each core with 2 threads per core.

\section{Accelerate}
\label{sec:accelerate}
Accelerate is an embedded purely functional array language in Haskell \cite{chakravarty2011accelerating}.
Accelerate has a frontend containing the embedded language, and the backend which handles code generation and execution.
The frontend handles general optimizations such as sharing recovery and array fusion \cite{mcdonell2013optimising,balen2020optimal}.
Further hardware specific optimization is handled on the various backends.
There are two LLVM \cite{llvm} backends provided: one that targets multicore CPUs \texttt{accelerate-llvm-native} and one that targets Nvidia GPUs \texttt{accelerate-llvm-ptx}.
In both backends we compile Accelerate code to LLVM IR.
When we want to run Accelerate on a GPU, LLVM will handle the compilation from LLVM IR to PTX, the instructions set for Nvidia's CUDA programming environment \cite{mcdonell2015type, llvm, nvidia2021cudadocs}.
The GPU backend implements a series of skeletons which implement primitive operations such as stencils, generate, permute, and scan.
These skeletons define how a program should be compiled and is the part where a custom thread scheduler can be implemented.
Further customizations to the scheduler can be done on the executing side of the backend as it controls how kernels are launched.

\section{Commonly Applicable Cache Improvements}

\subsection{Optimizations of Blocked Algorithms}
\label{sec:optimization_blocked}
\citeauthor{lam1991cache} expands on the well known idea of working on blocks instead of entire rows or columns.

If all data fits onto cache without eviction, the misses that occur are \textit{intrinsic misses}.
In the real world however, data can be evicted by other memory accesses.
This interference of reuse is categorized between two cases: \textit{cross interference} and \textit{self interference}.
Cross interference assumes the location of data in memory is unrelated to the location in cache, and instead is measured by probablity that the reuse falls within the footprint of the variable.
Self interference extends this by taking the cache locations of variables into account, which can happen when the data for a single iteration no longer fits in cache.
\cite{lam1991cache}

\subsection{CTA Clustering}
\citeauthor{li2017locality} presented a clustering algorithm for reorder threads to improve cache performance on GPU.
It replaces a kernel with a new one with predefined clustering rules.
CTAs with inter-CTA locality are clustered together which can then be assigned to SMs.
The work in these clusters can be bounded to SMs in two ways:
Round-Robin Binding which asumes the scheduler assigns CTAs to SMs in a strict Round Robin policy, and SM-based binding which extracts the current executing SM for a CTA which it uses to devide the work it will do.
The former results in redirection based clustering while the latter forms agent based clustering.

Additionally, \citeauthor{li2017locality} implemented three optimizations into their clustering framework: 
\textbf{i) CTA Throttling} which limits the number of concurrent CTAs on an SM to reduce resource contention.
\textbf{ii) Cache Bypassing} to avoid unnecessary cache pollution.
\textbf{iii) CTA Prefetching using Reshaped Order}

CTA clustering observed no significant speedup for normal and dense matrix multiplications.
For convolutional neural networks (similar memory access patterns as stencils), a $1.4\times$ speedup was observed for redirection based clustering, and a $1.2\times$ speedup for other clustering algorithms of the Fermi architecture.
However, on the Pascal architecture, convolutional neural networks no speedup was observed, and on Maxwell and Kepler this improvement is limited to $1.1\times$.

\subsection{PAVER}
\citeauthor{tripathy2021paver} presented a novel threadblock scheduling method \cite{tripathy2021paver} which analyses GPU code and generates a data dependency graph between threadblocks. 
A cache optimal program is then the clustering of threadblocks which have the least amount of dependency between clusters.
\citeauthor{tripathy2021paver} have observed a 20-40\% performance improvement on matrix multiplication depending on the architecture and configuration.
However, this method is not easily implementable and no publicly available implemention exists so far.
